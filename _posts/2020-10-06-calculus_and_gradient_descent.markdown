---
layout: post
title:      "Calculus and Gradient Descent"
date:       2020-10-07 03:59:32 +0000
permalink:  calculus_and_gradient_descent
---


The concepts of calculus and gradient descent can prove confusing if you do not have a math background, or you haven’t had to learn math in a long time. However under the sometimes overly complicated symbols and formulas, these concepts are very intuitive and provide great insight into how some machine learning models create their parameters. Often, the only thing that stands between most people and understanding math are their mistaken beliefs that they are bad at math or that the concepts are too complicated for them to understand. 
A derivative is the instantaneous rate of change for a function. This is the same as the slope of a curve. Each point on the curve has its own slope, visualized by drawing a short straight that touches the curve only at the chosen point. For a straight line, the derivative is the same for the entire function. When your function is increasing, the derivate is positive. If you think of slope, this makes sense. When you’re moving positively in the x direction and the y direction, your slope will be positive. Alternatively, when the function is decreasing, the derivative is negative. It follows therefore, that the derivative must change sign when the function goes from increasing to decreasing or from decreasing to increasing. This also means that at some point the derivative must be equal to zero. In fact when the derivative is zero, you have reached one of the maximum or minimum points of the function. These could be the maximum or minimum of a specific range (called the local max or min) or the maximum or minimum of the whole function (called the global max or min). 
This brings us to the cost curve. This is just a function representing your error as you vary the parameters of your model. Gradient descent aims to minimize the cost curve. With one parameter, this is easy to visualize. You want to slide down the curve until you reach the flattest point. This becomes a little harder when you take into account multiple parameters. This is because you can change coefficients for as many variables as you have in your model. That means you have to move a different amount for each parameter in order to get as far down the cost curve as you can in each step. 
I snowboard, so I like to visualize this concept on top of a mountain. Let’s say you need to get to the bottom of the mountain as quickly as possible to meet someone. The steepest trail at the top might be to your left. So you point your board down, left, and forward and start moving. The mountain won’t be exactly the same in all places, so you use the tilt of the board to find the steepest point at any moment. Sometimes that means going straight down, other time you might have to take a sharp right or left, and in others you might have to move completely to the right or left without going down. 
So how do you calculate how much to move in each direction? You have to compute the partial derivative for each parameter. When you take a partial derivative, you’re only taking into account the change in that specific variable. All other variables are treated as constants. Remember that derivatives are 

