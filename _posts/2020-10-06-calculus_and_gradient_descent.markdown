---
layout: post
title:      "Calculus and Gradient Descent"
date:       2020-10-06 23:59:32 -0400
permalink:  calculus_and_gradient_descent
---


The concepts of calculus and gradient descent can prove confusing if you do not have a math background, or you haven’t had to learn math in a long time. However under the sometimes overly complicated symbols and formulas, these concepts are very intuitive and provide great insight into how some machine learning models create their parameters. Often, the only thing that stands between most people and understanding math are their mistaken beliefs that they are bad at math or that the concepts are too complicated for them to understand. 

A derivative is the instantaneous rate of change for a function. This is the same as the slope of a curve. Now it is a small but important thing to note that instantaneous rate of change is slightly misleading. Technically the instantaneous rate of change is like taking a screenshot of a video. There is actually no change at a single instance, just as there is no movement in a photograph. The derivative is more like an iPhone’s live photo option. You take a picture and it looks like a picture but when you hold the photo down you realize there is a small amount of movement. That small amount of movement is the derivative. So the derivative at any given point is really the rate of change over a tiny interval that includes that point. That is why the mathematical formula for the derivative includes a limit going to zero. 

f' (x)=lim(h→0)⁡〖(f(x+h)- f(x))/h〗

This formula is simply defining the interval as x to x+h and then trying to make h as small as possible to create as small of an interval as possible. However h cannot be zero, i.e. the rate of change cannot be completely instantaneous, because you cannot divide by zero. However instantaneous rate of change is more intuitive and captures what the derivative is really doing, even if this definition is slightly off in a technical sense. 

Each point on the curve has its own slope, visualized by drawing a short straight that touches the curve only at the chosen point. For a straight line, the derivative is the same for the entire function.  For a constant line, there is no change, making the derivative zero. When your function is increasing, the derivate is positive. If you think of slope, this makes sense. When you’re moving positively in the x direction and the y direction, your slope will be positive. 
Alternatively, when the function is decreasing, the derivative is negative. It follows therefore, that the derivative must change sign when the function goes from increasing to decreasing or from decreasing to increasing. This also means that at some point the derivative must be equal to zero. In fact when the derivative is zero, you have reached one of the maximum or minimum points of the function. These could be the maximum or minimum of a specific range (called the local max or min) or the maximum or minimum of the whole function (called the global max or min). 

This brings us to the cost curve. This is just a function representing your error as you vary the parameters of your model. Gradient descent aims to minimize the cost curve. With one parameter, this is easy to visualize. You want to slide down the curve until you reach the flattest point. This becomes a little harder when you take into account multiple parameters. This is because you can change coefficients for as many variables as you have in your model. That means you have to move a different amount for each parameter in order to get as far down the cost curve as you can in each step. 

I snowboard, so I like to visualize this concept on top of a mountain. Let’s say you need to get to the bottom of the mountain as quickly as possible to meet someone. The steepest trail at the top might be to your left. So you point your board down, left, and forward and start moving. The mountain won’t be exactly the same in all places, so you use the tilt of the board to find the steepest point at any moment. Sometimes that means going straight down, other time you might have to take a sharp turn in the left direction where you are only going down slightly, and in others you might have to move completely to left without going down. 

So how do you calculate how much to move in each direction? You have to compute the partial derivative for each parameter. When you take a partial derivative, you’re only taking into account the change in that specific variable. All other variables are treated as constants. Remember that derivatives are the instantaneous rate of change. So if all we care about is the change in the left direction, we’re by definition looking at change in the left direction. So the change in down is zero, making it just like any other constant. After each instant, we are calculating how much we changed in each direction. That gives us a new position in terms of left and down. Then we calculate the change in these directions that gets us down the farthest in the next instant. And so on. 

However there is one problem right now. If we keep covering such big distances on every instant, and assuming we aren’t very good at stopping when we reach the bottom of the mountain, we’re probably going to sale right past the person we’re meeting. In terms of gradient descent, we’re taking too big of a step and we’ll miss the minimum completely and end up going back and forth from the increasing to the decreasing side of the cost curve. This is why a learning rate is used in gradient descent. We want to make bigger progress when we’re further from the minimum and make progressively smaller changes as we get closer. This involves multiplying all of the partial derivatives by the same value. So no learning rate would be a very well-waxed snowboard that is able to go faster, as opposed to a less-waxed board that would slow down more as the terrain gets flatter.  
 
 In summary, gradient descent, and the calculus behind it is highly intuitive despite how intimidating it might look at first. 


