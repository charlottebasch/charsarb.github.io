---
layout: post
title:      "Data Cleaning "
date:       2020-08-29 03:52:00 +0000
permalink:  data_cleaning
---


    Data cleaning is a critical first step in beginning your model. I find things easiest when there is no limit to the number of rows and columns pandas can display. This can be done by setting the display.max_rows and display.max_columns to none so that the data frame will not be cut off. The easiest place to start is to view to head of your data frame, which we will refer to as df. Once you look at your columns and the kind of data in them, you’ll have a better sense if something is misclassified. You can also view the df.shape to find the number of rows and columns. Using df.describe() can tell you numerical information about the columns, such as the minimum, maximum, mean, and median. To further understand your dataset, view the df.info(). This will tell you the data types of each column. If you notice a column where the name and data type do not seem to correspond, such as a price that is an object, you will need to change the data type. In the case of money, this may involve removing extra characters such as dollar signs or commas. However a numerical value may also be misclassified because a character such as a question mark is being used to mark missing values. To identify if this is the case view the value counts. I also print out the number of unique values using nunique for each column to get a sense of the scope of the data. I prefer to view only the first five, since a missing value is often one of the most common. Also be on the lookout for numeric values that do not make sense, such as -1 for age, which may also indicate missing values. Replace these values with NaNs using with df.replace and replace the out of place value with numpy’s np.nan.
    Next use df.isnull().sum() to view the number of not a number values for each column. You must decide whether to keep or discard the missing values. If most of the values in column are missing, you may drop the column using df.drop(cols_to_drop, axis=1). If less than 5% of the rows contain missing data, you can drop the rows with missing data df.dropna (subset=cols_with_rows_to_drop). However if the percentage of missing values in somewhere in the middle, you will have to replace the missing data. For categorical columns, you can replace NaN values with “missing” or some other phrase to indicate the data were missing, the most common value, or randomly fill in the values based on the percentages of the other column values. I prefer to create a separate “missing” category because there might be something different about the data with missing values in this column. For quantitative data, you can replace the missing values with a measure of central tendency or bin the data, transforming it into a categorical variable. I prefer to replace missing values with the median. This can be done using df.fillna. At this point you can change the data types of any misclassified columns using df[col].astype(data_type). 
    Now you can start modifying your columns if necessary. For instance if you’d like to split your data from one column into multiple columns you can use string splitting and then assign each part of the string to a new column. If you do this for numerical data, remember to convert it back to a numerical format as the string splitting has turned it into an object. You can also do mathematical operations on your columns. For example, in this project I subtracted the year a house was built from the year it was sold to come up with a specific column for the house’s age when it was sold. This is also the time to decide if numerical columns with few values should become categorical. For example, in this dataset there were very few houses that had a year renovated. Hence I decided to simply create a new column to indicate whether or not a house had been renovated. This was accomplished by mapping a lambda function where the value was zero if the year renovated column was zero and one if the year renovated column had any other value. Then either drop the columns you are no longer interested in from the data frame or create a copy of the data frame under a new name with the columns dropped. It becomes useful later to have a list with the names of both categorical and numerical columns, so this is the point where I create one. I also create a general predictors data frame, which is a copy of the original data frame with the outcome variable dropped. 
    Also at this point transformations, such as log transformations, can be applied to a data frame. However this can also be kept until an initial model indicates this to be a necessary choice. If log transformation – for example – is done at this point, scaling can be done as well. However if you decide to wait until later to log transform or do another transformation where division is involved, you should not standardize at this point. This is because you cannot take the log transformation of a number less than one and cannot divide by zero and scaling will produce such numbers. Of course if you do not plan on transforming your data, then this would be an appropriate time to standardize it. You can also get rid of outliers at this point. For a more conservative approach to outlier removal, you can use z-scores, where values with an absolute value of their z-score greater than three are eliminated. For a less conservative approach you can use the interquartile range (IQR) to eliminate outliers. This method only keeps values that are between the first quartile minus 1.5 times the IQR and the third quartile plus 1.5 times the IQR. Whether to be more conservative with outlier elimination depends of the model and data in question. Following these steps will you up for a successful model building experience. 

